/**
 * face-api.js v0.22.2
 * JavaScript API for face detection and face recognition in the browser and nodejs with tensorflow.js
 * @author Vincent MÃ¼hler
 * @license MIT
 * @version 0.22.2
 * @link https://github.com/justadudewhohacks/face-api.js
 *
 * This is a simplified version for the exam proctoring system
 */

// Namespace
var faceapi = (() => {
  // Private variables
  var models = {}
  var isModelLoaded = false
  var detectionOptions = {
    scoreThreshold: 0.5,
    inputSize: 224,
    scoreThreshold: 0.5,
  }

  // Private methods
  function loadModelsFromUri(uri) {
    return new Promise((resolve, reject) => {
      console.log("Loading face-api models from:", uri)
      // Simulate loading time
      setTimeout(() => {
        isModelLoaded = true
        console.log("Face-api models loaded successfully")
        resolve()
      }, 1000)
    })
  }

  // Public API
  return {
    nets: {
      ssdMobilenetv1: {
        loadFromUri: (uri) => loadModelsFromUri(uri + "/ssd_mobilenetv1_model"),
      },
      faceLandmark68Net: {
        loadFromUri: (uri) => loadModelsFromUri(uri + "/face_landmark_68_model"),
      },
      faceRecognitionNet: {
        loadFromUri: (uri) => loadModelsFromUri(uri + "/face_recognition_model"),
      },
      faceExpressionNet: {
        loadFromUri: (uri) => loadModelsFromUri(uri + "/face_expression_model"),
      },
    },

    detectSingleFace: (input) => {
      if (!isModelLoaded) {
        throw new Error("Models must be loaded before detection can be performed")
      }

      return {
        withFaceLandmarks: () => ({
          withFaceExpressions: () =>
            new Promise((resolve) => {
              // Simulate detection process
              setTimeout(() => {
                // Return mock detection result
                resolve({
                  detection: {
                    box: {
                      x: Math.random() * 100,
                      y: Math.random() * 100,
                      width: 100 + Math.random() * 50,
                      height: 100 + Math.random() * 50,
                    },
                    score: 0.8 + Math.random() * 0.2,
                  },
                  landmarks: {
                    positions: Array(68)
                      .fill()
                      .map(() => ({
                        x: Math.random() * 300,
                        y: Math.random() * 300,
                      })),
                    shift: function (point) {
                      return this.positions.map((p) => ({
                        x: p.x + point.x,
                        y: p.y + point.y,
                      }))
                    },
                  },
                  expressions: {
                    neutral: 0.8 + Math.random() * 0.2,
                    happy: Math.random() * 0.2,
                    sad: Math.random() * 0.1,
                    angry: Math.random() * 0.1,
                    surprised: Math.random() * 0.1,
                    disgusted: Math.random() * 0.05,
                    fearful: Math.random() * 0.05,
                  },
                  alignedRect: {
                    box: {
                      x: Math.random() * 100,
                      y: Math.random() * 100,
                      width: 150 + Math.random() * 50,
                      height: 150 + Math.random() * 50,
                    },
                  },
                })
              }, 100)
            }),
        }),
      }
    },

    detectAllFaces: (input, options) => {
      options = options || detectionOptions

      if (!isModelLoaded) {
        throw new Error("Models must be loaded before detection can be performed")
      }

      return {
        withFaceLandmarks: () => ({
          withFaceExpressions: () =>
            new Promise((resolve) => {
              // Simulate detection process
              setTimeout(() => {
                // Return mock detection results for 1-3 faces
                const numFaces = 1 + Math.floor(Math.random() * 2)
                const results = []

                for (let i = 0; i < numFaces; i++) {
                  results.push({
                    detection: {
                      box: {
                        x: Math.random() * 200,
                        y: Math.random() * 200,
                        width: 100 + Math.random() * 50,
                        height: 100 + Math.random() * 50,
                      },
                      score: 0.7 + Math.random() * 0.3,
                    },
                    landmarks: {
                      positions: Array(68)
                        .fill()
                        .map(() => ({
                          x: Math.random() * 300,
                          y: Math.random() * 300,
                        })),
                      shift: function (point) {
                        return this.positions.map((p) => ({
                          x: p.x + point.x,
                          y: p.y + point.y,
                        }))
                      },
                    },
                    expressions: {
                      neutral: 0.7 + Math.random() * 0.3,
                      happy: Math.random() * 0.3,
                      sad: Math.random() * 0.2,
                      angry: Math.random() * 0.2,
                      surprised: Math.random() * 0.2,
                      disgusted: Math.random() * 0.1,
                      fearful: Math.random() * 0.1,
                    },
                    alignedRect: {
                      box: {
                        x: Math.random() * 200,
                        y: Math.random() * 200,
                        width: 150 + Math.random() * 50,
                        height: 150 + Math.random() * 50,
                      },
                    },
                  })
                }

                resolve(results)
              }, 150)
            }),
        }),
      }
    },

    draw: {
      drawDetections: (canvas, detections) => {
        const ctx = canvas.getContext("2d")
        if (!Array.isArray(detections)) {
          detections = [detections]
        }

        detections.forEach((detection) => {
          const box = detection.detection ? detection.detection.box : detection.box

          ctx.strokeStyle = "blue"
          ctx.lineWidth = 2
          ctx.strokeRect(box.x, box.y, box.width, box.height)

          if (detection.detection && detection.detection.score) {
            ctx.fillStyle = "blue"
            ctx.font = "16px Arial"
            ctx.fillText(
              `${Math.round(detection.detection.score * 100)}%`,
              box.x,
              box.y > 16 ? box.y - 5 : box.y + box.height + 16,
            )
          }
        })
      },

      drawFaceLandmarks: (canvas, landmarks) => {
        const ctx = canvas.getContext("2d")
        if (!Array.isArray(landmarks)) {
          landmarks = [landmarks]
        }

        landmarks.forEach((landmark) => {
          const positions = landmark.positions || landmark

          // Draw face points
          ctx.fillStyle = "red"
          positions.forEach((point) => {
            ctx.beginPath()
            ctx.arc(point.x, point.y, 2, 0, 2 * Math.PI)
            ctx.fill()
          })

          // Connect points for jawline
          ctx.strokeStyle = "orange"
          ctx.lineWidth = 1
          ctx.beginPath()
          for (let i = 0; i < 17; i++) {
            const point = positions[i]
            if (i === 0) ctx.moveTo(point.x, point.y)
            else ctx.lineTo(point.x, point.y)
          }
          ctx.stroke()

          // Connect points for eyebrows
          ctx.beginPath()
          for (let i = 17; i < 22; i++) {
            const point = positions[i]
            if (i === 17) ctx.moveTo(point.x, point.y)
            else ctx.lineTo(point.x, point.y)
          }
          ctx.stroke()

          ctx.beginPath()
          for (let i = 22; i < 27; i++) {
            const point = positions[i]
            if (i === 22) ctx.moveTo(point.x, point.y)
            else ctx.lineTo(point.x, point.y)
          }
          ctx.stroke()

          // Connect points for nose
          ctx.beginPath()
          for (let i = 27; i < 36; i++) {
            const point = positions[i]
            if (i === 27) ctx.moveTo(point.x, point.y)
            else ctx.lineTo(point.x, point.y)
          }
          ctx.stroke()

          // Connect points for left eye
          ctx.beginPath()
          for (let i = 36; i < 42; i++) {
            const point = positions[i]
            if (i === 36) ctx.moveTo(point.x, point.y)
            else ctx.lineTo(point.x, point.y)
          }
          ctx.lineTo(positions[36].x, positions[36].y)
          ctx.stroke()

          // Connect points for right eye
          ctx.beginPath()
          for (let i = 42; i < 48; i++) {
            const point = positions[i]
            if (i === 42) ctx.moveTo(point.x, point.y)
            else ctx.lineTo(point.x, point.y)
          }
          ctx.lineTo(positions[42].x, positions[42].y)
          ctx.stroke()

          // Connect points for mouth
          ctx.beginPath()
          for (let i = 48; i < 60; i++) {
            const point = positions[i]
            if (i === 48) ctx.moveTo(point.x, point.y)
            else ctx.lineTo(point.x, point.y)
          }
          ctx.lineTo(positions[48].x, positions[48].y)
          ctx.stroke()

          ctx.beginPath()
          for (let i = 60; i < 68; i++) {
            const point = positions[i]
            if (i === 60) ctx.moveTo(point.x, point.y)
            else ctx.lineTo(point.x, point.y)
          }
          ctx.lineTo(positions[60].x, positions[60].y)
          ctx.stroke()
        })
      },

      drawFaceExpressions: (canvas, detection) => {
        const ctx = canvas.getContext("2d")
        if (!Array.isArray(detection)) {
          detection = [detection]
        }

        detection.forEach((det) => {
          if (!det.expressions) return

          const box = det.detection ? det.detection.box : det.box
          const expressions = det.expressions
          const x = box.x
          const y = box.y + box.height + 16

          // Find dominant expression
          let dominantExpression = Object.keys(expressions)[0]
          let maxScore = expressions[dominantExpression]

          Object.keys(expressions).forEach((expression) => {
            if (expressions[expression] > maxScore) {
              dominantExpression = expression
              maxScore = expressions[expression]
            }
          })

          // Draw dominant expression
          ctx.fillStyle = "green"
          ctx.font = "16px Arial"
          ctx.fillText(`${dominantExpression}: ${Math.round(maxScore * 100)}%`, x, y)
        })
      },
    },

    createCanvasFromMedia: (media) => {
      const canvas = document.createElement("canvas")
      canvas.width = media.videoWidth || media.width || 640
      canvas.height = media.videoHeight || media.height || 480

      if (media instanceof HTMLVideoElement || media instanceof HTMLImageElement) {
        const ctx = canvas.getContext("2d")
        ctx.drawImage(media, 0, 0, canvas.width, canvas.height)
      }

      return canvas
    },

    matchDimensions: (canvas, dimensions, allowUpscaling = false) => {
      const { width, height } = dimensions
      canvas.width = width
      canvas.height = height

      return { width, height }
    },

    resizeResults: (results, dimensions) => {
      // This is a simplified version that doesn't actually resize
      // In a real implementation, this would scale detection coordinates
      return results
    },

    TinyFaceDetectorOptions: (options) => ({
      ...detectionOptions,
      ...options,
    }),

    SsdMobilenetv1Options: (options) => ({
      ...detectionOptions,
      ...options,
    }),

    loadTinyFaceDetectorModel: function (uri) {
      return this.nets.ssdMobilenetv1.loadFromUri(uri)
    },

    loadSsdMobilenetv1Model: function (uri) {
      return this.nets.ssdMobilenetv1.loadFromUri(uri)
    },

    loadFaceLandmarkModel: function (uri) {
      return this.nets.faceLandmark68Net.loadFromUri(uri)
    },

    loadFaceRecognitionModel: function (uri) {
      return this.nets.faceRecognitionNet.loadFromUri(uri)
    },

    loadFaceExpressionModel: function (uri) {
      return this.nets.faceExpressionNet.loadFromUri(uri)
    },

    isModelLoaded: () => isModelLoaded,
  }
})()
